{
 "cells": [
  {
   "cell_type": "raw",
   "id": "fb45a7af",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Working with data science\"\n",
    "author: \"donata boric\"\n",
    "format:\n",
    "  html:\n",
    "    toc: true\n",
    "    toc-title: On this page\n",
    "    code-fold: true\n",
    "    code-tools: true\n",
    "    smooth-scroll: true\n",
    "  docx:\n",
    "    toc: true\n",
    "    toc-title: On this page\n",
    "jupyter: python3\n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a799f87",
   "metadata": {},
   "source": [
    "## DATA SCIENCE\n",
    "Data science combines statistics, maths, specialised programs, AI, ML, etc.. Application of specific principles and analytic techniques to extract information from data used in strategic planning, decision making, etc.\n",
    "\n",
    "![](Picture3.png){width=40%, fig-align=\"center\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e4ff66",
   "metadata": {},
   "source": [
    "* **Handling missing values:** If the dataset is large, we can just remove rows with missing data. For smaller datasets, we can substitute it with the mean or average, using df.mean(), or df.fillna(mean),..\n",
    "* **Handling outliers:** if the data has garbage value or has extreme values you can drop it. Otherwise, you can maybe try different models, normalize data or use algorithms that are less affected by it\n",
    "* Difference between **error terms and residuals:** an error term is generally unobservable and a residual is observable and calculable, making it much easier to quantify and visualize. In effect, while an error term represents the way observed data differs from the actual population, a residual represents the way it differs from sample population data. Residuals actually helps us get an accurate estimate of the error.\n",
    "* **RMSE** is used to measure the deviation of the residuals, and MSE is used to find how close is the line to the actual data. Other metrics: MAE (mean absolute error), MAPE (mean absolute percentage error). \n",
    "    from sklearn.metrics import .......\n",
    "* **Cost vs loss function:** loss function refers to the error of one training example, while a cost function calculates the average error across an entire training set\n",
    "* **Confusion matrix** is used to describe the performance of the classification model.\n",
    "                     ![](Picture4.png){width=35%}\n",
    "    - **accuracy:** (true positive + true negative) / total observations.. RMSE is a measure of accuracy in regression\n",
    "    - **error rate:** (false positive + false negative) / total observations\n",
    "\t- **precision:** true positive / (true positive + false positive) \n",
    "\t- **specificity:** true negative / (true negative + false positive)\n",
    "    - **sensitivity/recall:** true positive / (true positive + false negative).. helps to identify the misclassified positive predicitions\n",
    "    \n",
    "    Accuracy metric can be reliable metric only if the dataset is class-balanced. F1 score is a ML evaluation metric that assesses the predictive skill of a model by elaborating on its class-wise performance rather than an overall performance as done by accuracy. If accuracy is 100%, then F1 = 1.\n",
    "    \n",
    "      F1 = 2 * precision * recall / (precision + recall)\n",
    "        \n",
    "    **Cut off/threshold** is the probability that the prediction is true. It represents the tradeoff between false positives and false negatives. Normally, the cut-off will be on 0.5 (random) but you can increase it. All predicted outcome with a probability above it will be classified in the first class and the other in the second class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95801ce6",
   "metadata": {},
   "source": [
    "### Statistics\n",
    "* **Deterministic vs stochastic process:** a deterministic process is a mathematical model where the output depends solely on the input, and there is no randomness involved. In contrast, a stochastic process is a mathematical model that involves randomness and is used to model situations that may not have inherent randomness. A deterministic model is completely predictable also.\n",
    "* **Unit root** is a feature of some stochastic processes. A linear stochastic process has a unit root if 1 is a root of the process's characteristic equation. Such a process is non-stationary. If the other roots of the characteristic equation lie inside the unit circle, then the first difference of the process will be stationary; otherwise, the process will need to be differenced multiple times to become stationary.\n",
    "* **Bias** is a systematic tendency to underestimate or overestimate the value of a parameter (you were not random!). It implies that the data selection may have been skewed by the collection criteria (in favor or against an idea). It can also be defined as a systematic (built-in) error which makes all values wrong by a certain amount. In ML, the inability for a ML method to capture the true relationship is called bias, that happens because algorithm makes simplified assumptions so that it can easily understand the target function.\n",
    "\n",
    "| **Bias** | **Error** | \n",
    "|:-----:|:-----:|\n",
    "| Produces prejudiced results | Results in inaccurate outcomes |\n",
    "| Identified manually or through software packages | Identified through calculations |\n",
    "| Occurs systematically | Occurs randomly |\n",
    "\n",
    "* **Normal/uniform distribution** is the kind of distribution that has no bias either to the left or to the right and is in the form of a bell-shaped curve. In this distribution, mean is equal to the median.\n",
    "* **Skewed distribution** is a distribution where the curve is inclined towards one side.\n",
    "* **Variance** is a statistical measurement used to determine the average of each point from the mean (the average of the squared differences from the mean). In ML the difference in fits between training and test sets is called variance, i.e. it refers to the changes in the model when using different portions of the training data set. Simply put, variance is the variability in the model prediction. **Standard deviation** is the spread of a group of numbers from the mean.\n",
    "\n",
    "$$ Var(X) = E[X^2] â€“ E[X]^2 $$\n",
    "\n",
    "| **Signs of high bias ML model** | **Signs of high variance ML model** | \n",
    "|:-----:|:-----:|\n",
    "| Failure to capture data trends | Noise in data set |\n",
    "| Underfitting | Overfitting |\n",
    "| Overly simplified | Complexity |\n",
    "| High error rate | Forcing data points together |\n",
    "\n",
    "* **Robustness** represents the system's capability to handle differences and variances effectively\n",
    "* Total error = variance + bias + irreducible error\n",
    "\n",
    "![](Picture5.png){width=110%, fig-align=\"center\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a82ddb8",
   "metadata": {},
   "source": [
    "* **Correlation vs covariance:** correlation is a measure of relationship between two variables and says how strong are the variables related. Range is -1 to 1. Covariance represents the extent to which the variables change together in a cycle. This explains the systematic relationship between pair of variables where changes in one affect changes in another variable. Range is -inf to +inf, and is affected by scalability.\n",
    "* **Confounding variables** are extraneous variables in a statistical model that correlates directly or inversely with both the dependent and the independent variable. Left unchecked, confounding variables can introduce many research biases to your work, causing you to misinterpret your results.\n",
    "* **R-squared/coefficient of determination** is a statistical measure in a linear regression model that determines the proportion (percentage) of the variance in the dependent variable that can be explained by the independent variable. In other words, it evaluates the scatter of the data points around the fitted regression line, i.e. shows how well the regression model explains observed data.\n",
    "![](Picture6.png){fig-align=\"center\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c4f718",
   "metadata": {},
   "source": [
    "* **p-value** is the measure of the statistical importance of an observation. We compute the p-value to understand whether the given data really describes the observed effect or not. If p<=0.05 it suggests that there is only 5% chance that the outcomes of an experiment are random and the null hypothesis must be rejected.\n",
    "$$ p_{value} = P(E|H_{0}) $$\n",
    "* **The statistical power** of a binary hypothesis test is the probability that the test rejects the null hypothesis when a specific alternative hypothesis is true\n",
    "* **Confidence interval** is a range of values likely containing the population parameter. **Confidence level** is denoted by 1-alpha, where alpha is level of significance (usually 5%). **Point estimate** is an estimate of the population parameter (can be derived with Maximum Likelihood estimator for egz.)\n",
    "![](Picture7.png){fig-align=\"center\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6450b33f",
   "metadata": {},
   "source": [
    "* **Univariate, bivariate and multivariate analysis:** univariate analysis allows us to understand the data and extract patterns and trends out of it. Bivariate analysis allows us to figure out the relationship between the variables. Multivariate analysis allows us to figure out the effects of all other variables (input variables) on a single variable (the output variable).\n",
    "* **Sampling** is the selection of individual members or a subset of the population to estimate the characters of the whole population. It is useful with datasets that are too large to efficiently analyze in full. There are two types of sampling techniques: probability and non-probability. **Resampling** is the process of changing/exchanging data samples, identifying the impact of these changes on model and prediction characteristics, and continuing until optimal results are achieved. It is done in cases of estimating the accuracy of sample statistics or validating models by using random subsets to ensure variations are handled (egz. Bootstraping, cross-validation)\n",
    "\n",
    "    Types of biases that can occur during sampling: selection bias, undercoverage bias and survivorship bias. **Selection bias** occurs when a sample selection does not accurately reflect the target population. **Survivorship bias** is the logical error of focusing on aspects that support surviving a process and casually overlooking those that did not. This can lead to wrong conclusions in numerous ways.\n",
    "* **Bootstrap method** is a resampling method by independently sampling with replacement from an existing sample data with same sample size n, and performing inference among these resampled data.\n",
    "* **Normalization/Min-Max scaling** is used to transform features to be on a similar scale ([0,1] or [-1,1]). It is useful when there are no outliers.\n",
    "$$ X_{new} = (X - X_{min}) / (X_{max} - X_{min}) $$\n",
    "* **Standardization/Z-score** normalization is the transformation of features by subtracting from mean and dividing by standard deviation. It is not affected with outliers since its not bounded to a certain range\n",
    "* **The Central limit theorem** says that, given a large enough sample size, the distirbution of sample averages/means will be approximtely normal. This means that we can use normal distirbution to make predictions about populations based on samples.\n",
    "* **The Law of large numbers** is a theorem that describes the result of performing the same experiment very frequently. It states that the sample mean, sample variance, and sample standard deviation converge to what they are trying to estimate\n",
    "* **Gradient**, for purposes of this paper, measures how much the output of a function changes if you change the inputs a little bit (from a given point). \n",
    "* **Categorical, continuous and ordinal variables.** An ordinal variable is a categorical variable for which the possible values are ordered.\n",
    "* **Histrograms vs boxplots:** Boxplots are more often used in comparing several datasets and take less space than histograms. Histograms are used to know and understand the probability distribution underlying a dataset\n",
    "* **Histograms vs bar graphs:** a bar graph is the graphical representation of categorical data, whereas a histogram is the graphical representation of data where data is grouped into continuous number ranges\n",
    "* **Kernel density estimation (KDE)** is a method for visualizing the distirbution of observations in  dataset over a continuous interval or time period\n",
    "* **Histograms vs density plots:** an advantage that density plots have over histograms is that they're better at determining the distribution shape because they are not affected by the number of bins used.\n",
    "* **Akaike information Criterion (AIC):** is a mathematical method for evaluating how well a model fits the data it was generated from. In statistics, it is used to compare different possible models (model selection). Lower AIC scores are better!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf53245",
   "metadata": {},
   "source": [
    "### Time series analysis\n",
    "Time series analysis (TSA) is a mathematical approach for predicting or forecasting the future pattern of data using historical data arranged in a successive order for a particular time period. *statsmodels.tsa* package contains model classes and functions that are useful for time series analysis.\n",
    "\n",
    "* **Prediction vs forecasting:** prediction is concerned with estimating the outcomes of unseen data. Forecasting is a sub-discipline of prediction in which we are making predictions about the future on the basis of time series data, so the only difference is that we consider the temporal dimension\n",
    "* **Trend vs season vs cyclic:** A trend exists when there is a long-term increase or decrease in the data. It does not have to be linear. A seasonal pattern occurs when a time series is affected by seasonal factors such as the time of the year or the day of the week. Seasonality is always of a fixed and known frequency. A cycle occurs when the data exhibit rises and falls that are not of a fixed frequency.\n",
    "* **Rolling average/ moving average** is a metric that calculates trends over short periods of time using a set of data. Is uses smaller parts of the data and then rolls or moves for each new period. Calculating next rolling period involves leaving off your earliest unit and adding in your next unit. For egz., if you want to track down monthly data, take 12-month rolling period. After calculating average of 12 months, leave first month and add new month, then calculate average again for new rolling period. In that way, rolling period keeps moving.\n",
    "* **Augmented Dickey-Fuller test:** tests the null hypothesis that a unit root is present in a time series sample. It is a negative number, and the more negative it is, the stronger the rejection of the hypothesis that there is a unit root at some level of confidence.\n",
    "\n",
    "    There are 3 main versions of the test (Dickey-Fuller test is presented for simplicity):\n",
    "    1. Test for a unit root: $\\Delta y_{t} = \\delta y_{t-1} + u_{t} \\quad(u_{t} \\text{ is error term})$\n",
    "    2. Test for a unit root with constant: $\\Delta y_{t} = a_{0} + \\delta y_{t-1} + u_{t}$\n",
    "    3. Test for a unit root with constant and deterministic time trend: \n",
    "            $\\Delta y_{t} = a_{0} + a_{1}t + \\delta y_{t-1} + u_{t}$\n",
    "            \n",
    "    -> Hypothesis: H0: Î´ = 0 (process is not stationary)\n",
    "\t\t\t       H1: Î´ < 0 (process is stationary)\n",
    "                   \n",
    "    -> from statsmodels.tsa.stattools import adfuller. For additional parameters, it is the best practice to put autolag='AIC'. regression parameter has 4 parameters: 'c' for only constant (default), 'ct' for constant and trend, 'ctt' for constant and linear and quadratic trend, 'n' for no constant and no trend.\n",
    "    \n",
    "    -> Which version of test to choose? Î´ needs to be <= 0, so one way to find out is to see if it fits in the right interval. Other way is to compare AIC values and choose lowest. Also by inspecting data we can assume which to choose, but the best way is to perform all 3 types and inspect results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8eafed8",
   "metadata": {},
   "source": [
    "* **Stationary time series:** the only assumption in TSA is that the data is â€žstationaryâ€œ. Data is stationary when the variance and mean of the series are constant with time, with no periodic component (independent of time influence).\n",
    "    - Check it with Augmented Dickey-Fuller test\n",
    "    - Trend can result in a varying mean over time, wheras seasonality can result in a changing variance over time, both which define a time series as being non-stationary. (stationary datasets are much easier to  model). \n",
    "    - **Differencing** is a widely used data transform for making time series data stationary. Notice that some temporal structures may still exist after performing a differencing operation, such as in the case of a nonlinear trend. The number of times that differencing is performed is called the difference order. DataFrame diff() function can be used.\n",
    "    \n",
    "* There are two popular types of non-stationary time series:\n",
    "    a. **Trend-stationarity time series** are those whose mean trend is deterministic. In other words, the mean of the time series changes over time but at a constant rate. The time series is not stationary in the strict sense, but it is stationary in the sense that the trend is stable and predictable\n",
    "    b. **Difference-stationarity time series** have a mean trend that is stochastic. In other words, the mean of the time series changes over time in a random pattern.\n",
    "\n",
    "* **Log transform:** time series with an exponential distribution can be made linear by taking the logarithm of the values. Log transforms are popular with time series data as they are effective at removing exponential variance\n",
    "* **Autocorrelation analysis** is used in detecting patterns and checking for randomness. The analysis involves looking at the Autocorrelation Function (ACF) and Partial Autocorrelation Function (PACF) plots. \n",
    "\n",
    "    - Autocorrelation is a mathematical representation of the degree of similarity between a given time series and a lagged version of itself over successive time intervals. ACF function measures and plots the average correlation between data points in time series and previous values of the series measured for different lag lenghts.\n",
    "    - Partial autocorrelation is similar to autocorrelation except that each partial correlation controls for any correlation between observations of a shorter lag length. For egz., at second lag, the PACF measures the correlation between data points at time â€žtâ€œ with data points at time â€žt-2â€œ, while the ACF measures the same correlation but after controlling for the correlation between data points at time â€žtâ€œ with those at time â€žt-1â€œ.\n",
    "    - *from statsmodels.graphics.tsaplots import plot_acf, plot_pacf*\n",
    "    - Stationarity of time series can be inspected with ACF plot (along with ADF test). In case the autocorrelations are positive for multiple lags, the series requires further differencing; but if lag 1 autocorrelated itself pretty negatively, then the series is possibly over-differenced\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636a0d75",
   "metadata": {},
   "source": [
    "#### Models\n",
    "* **AutoRegressive model (AR):** it is a linear model where current period values are a sum of past outcomes multiplied by a numeric factor. We denote it as AR(p), where â€žpâ€œ is called the order of the model and represents the number of lagged values we want to include. p can be determined from PACF plot. For p=1:\n",
    "$$ X_{t} = C + \\phi_{1}X_{t-1} + \\varepsilon_{t}, $$\n",
    "    The coefficient $\\phi_{1}$ is a numeric constant with value between -1 and 1. When multiplied with past value it represents a part which remains in the future. You would choose an AR model if you believe that previous observations have a direct effect on the time series. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f696ec",
   "metadata": {},
   "source": [
    "* **Moving Average (MA):** it's a statistic that captures the average change in data series over time. We denote it as MA(q), where â€žqâ€œ is called the order of the model and represents the number of past forecast errors (or the size of the moving average window).  q can be determined from ACF plot. You would choose an MA model if you believe that the errors have a direct effect on the time series.\n",
    "* **AutoRegressive Moving Average (ARMA):** p,q\n",
    "* **AutoRegressive Integrated Moving Average (ARIMA):** p,d,q.. where d is the difference order\n",
    "* **AutoRegressive Moving Average with eXogeneous factors (ARMAX):** exogeneous variables are external data used in forecast (external effects)\n",
    "* **Seasonal AutoRegressive Integrated Moving Average (SARIMA):** p,d,q,P,D,Q,m.. where m is the number of time steps for a single seasonal period, p,d,q are trend elements and P,D,Q are seasonal elements\n",
    "* **Seasonal AutoRegressive Integrated Moving Average with eXogeneous factors (SARIMAX)**\n",
    "\n",
    "    STEPS FOR BUILDING ONE OF THESE MODELS:\n",
    "    1)\tCheck for stationarity of time series and perform differencing if needed. This is because the term â€žautoregressiveâ€œ implies Linear Regression model (using its lags as predictors) and it works well for independent and non-correlated predictors \n",
    "    2)\tDetermine parameters. It can be done with inspecting acf/pacf plots\n",
    "    3)\tFit the model. Inspect coefficients and P(>|z|) with .summary() function and decide if it is needed for further tuning of parameters\n",
    "    4)\tCheck residuals for making sure model has captured adequte information from the data (they should look like white noise). If density looks normally distirbuted, model is ready.\n",
    "    5)\tMake predictions (using .forecast() or .predict() function)\n",
    "    6)\tEvaluate model predictions using common metrics (MAE, RMSE,..)\n",
    "\n",
    "        - Alternatively, use *pmdarima* package and *auto_arima* function to automate steps 1 to 3. Be aware that sometimes the manually fitted model is closer to the actual test set\n",
    "        - Alternatively, use plot_diagnostics to automate step 4. Values of good fit:\n",
    "            a. Standardized residual: there are no obvious patterns in residuals, with values having a mean of zero\n",
    "            b. The KDE curve should be very similar to the normal distribution\n",
    "            c. Normal Q-Q: most of the data points should lie on the straight line\n",
    "            d. Correlogram: 95% of correlations for lag greater than zero should not be significant\n",
    "\n",
    "        - Suggestion: conduct time series cross-validation to select the best model, i.e. repeat model assessment for different train / test sets\n",
    "        - Pro tip: if data shows exponential trend you can do a log transform before applying a model, then later apply inverse transformation (exponential function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9671b473",
   "metadata": {},
   "source": [
    "##### Useful tips/functions\n",
    "* Date increment used for a date range: *pandas.tseries.offsets.DateOffset*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c38cea",
   "metadata": {},
   "source": [
    "### Machine learning\n",
    "Hypothesis in the content of ML represents a mathematical function that an algorithm uses to represent the relationship between the target variable and features. Learnings can be divided into supervised and unsupervised learning.\n",
    "\n",
    "| **Supervised learning** | **Unsupervised learning** | \n",
    "|:-----:|:-----:|\n",
    "| Works on the data that contains both inputs and the expected output, i.e. the labeled data | Works on the data that contains no mappings from input to output, i.e. the unlabeled data |\n",
    "| Used to create models that can be employed to predict or classify things | Used to extract meaningful information out of large volumes of data |\n",
    "| Egz. Decision trees, logistic regression, svm | Egz. K-means clustering, hierarchical clustering, apriori algorithm |\n",
    "\n",
    "* **Test set vs validation set:** test set is used to evaluate the performance of the trained model, and the validation set is part of the training set that is used to select parameters for avoiding model overfitting. (80% training, 10% validation, 10% test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db923876",
   "metadata": {},
   "source": [
    "#### Regression and classification algorithms\n",
    "* **Regression:** linear (when variables are continuous and numeric) and logistic (when variables are continuous and categorical)\n",
    "     -\t**Linear regression** is supervised learning algorithm, which helps in finding the linear relationship between two variables. It finds the smallest sum of squared residuals that is possible for the dataset.\n",
    "     \n",
    "* **Classification** refers to a predictive modeling process where a class label is predicted for a given example of input data. It helps categorize the provided input into a label that other observations with similar features have.\n",
    "     - **Naive Bayes** is supervised classification ML algorithm based on the Bayes theorem, which deals with the probability of an event occuring given that another event has already occured (i.e. mathematical formula for determining conditional probability). It is based on two assumptions, first, each feature/attribute present in the dataset is independent of another, and second, each feature carries equal importance. It has â€žnaiveâ€œ in it because it assumes that the occurence of a certain feature is independent of the occurence of other features (hence each feature individually contributes to identify the result), which is unrealistic for real-world data\n",
    "     - **Support vector machine (SVM)** is a supervised ML model that considers the classification algorithms for two-group classification problems. It is a representation of the training data as points in space that are seperated into categories with the help of a clear gap that should be as wide as possible. **Kernel function** is used to transform the data that is not linearly separable into one that is. It is generalized dot product function used for the computing dot product of vectors xx and yy in high dimensional feature space. This transformation is based on kernel trick (projecting data onto a higher dimension space where it can be linearly divided by a plane).\n",
    "     - **Logistic regression** is classification algorithm that is used to predict the probability of certain classes based on some dependent variables. Estimating probability is done by using its underlying logistic function (sigmoid). In short, the logistic regression model computes a sum of the input features and calculates the logistic of the result.\n",
    "     \n",
    "       Altough it is classiciation algorithm (it predicts a discrete class), it is part of the regression family as it involves predicting outcomes based on quantitative relationships between variables. Unlike, linear regression, it accepts both continuous and discrete variables as input and its output is qualitative. \n",
    "       \n",
    "       Sigmoid function: $\\quad S(x) = \\frac{1}{1 + e^{-x}} = \\frac{e^{x}}{e^{x} + 1} = 1 - S(-x).$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1913409b",
   "metadata": {},
   "source": [
    "* **Elbow method** is used to select â€žkâ€œ for k-means clustering. It plots the value of the cost function produced by different values of k (for egz. 1 to 15). k-means cost function is sum of squared distances of each data point to respective centroid of cluster to which the data points belong.\n",
    "* **A ROC curve** is a graph showing the performance of a classification model at all classification thresholds ([0,1]). It plots two parameters: true positive rate (sensitivity) and false positive rate (1-specificity). Also, decreasing the threshold moves up along the curve. Classifiers that give curves closer to the top-left corner indicate a better performance. Note that the ROC does not depend on the class distribution and this makes it useful for evaluating classifiers predicting rare events such as diseases or disasters. In contrast, evaluating performance using accuracy would favor classifiers that always predict a negative outcome for rare events. To compare different classifiers, it is useful to summarize the performance of each classifier into a single measure- **AUC** (area under the ROC curve). The AUC is the probability the model will score a randomly chosen positive class higher than a randomly chosen negative class.\n",
    "\n",
    "* CART is name for classification and regression trees\n",
    "* **Decision tree** is non-parametric model that can be used for both classification and regression. Non-parametric means that they don't increase their number of parameters as we add more features. They are constructed using nodes and branches, where the root node testes a feature which best splits the data. Decision trees are built by recursively splitting our training samples using the features from the data that work best for the specific task. The process is done by evaluating certain metrics (â€žinformation entropyâ€œ), depending if the feature is dicrete or continuous. \n",
    "\n",
    "    Steps are: \n",
    "    1. Take the entire data set as input\n",
    "\t2. Look for a split that maximizes the separation of the classes\n",
    "\t3. Apply the split (divide step)\n",
    "\t4. Re-apply steps 1) and 2) to the divided data\n",
    "\t5. Stop when you meet stopping criteria\n",
    "\t6. Pruning (clean up the tree if you went too far)\n",
    "\n",
    "    **Entropy** in ML is the measurement of disorder or impurities in the information processed.\n",
    "    \n",
    "    $$ E = -\\sum^{N}_{i=1}P_{i}\\text{log}_{2}P_{i}, $$\n",
    "    where $P_{i}$ is probability of randomly selecting an example in class i.\n",
    "    \n",
    "    **Information gain** is a measure of how much entropy is reduced when a particular feature is used to split the data. It calculates the difference between entropy before and after the split.\n",
    "\n",
    "    **Pruning** is a technique that simplifies the decision tree by reducing the rules. It helps to avoid the complexity and improves accuracy.\n",
    "\n",
    "* **Random forest** is a model built up of a number of decision trees. If you split the data into different packages and make a decision tree in each of the different groups of data. The random forest brings all those trees together (individual trees need to have low correlations with each other).\n",
    "\n",
    "    Steps to build a model:\n",
    "    1. Randomly select *k* features from a total of *m* features (k<<m)\n",
    "    2. Among the *k* features, calculate the node using the best split point\n",
    "    3. Split the node into daughter nodes using the best split\n",
    "    4. Repeat steps two and three until leaf nodes are finalized\n",
    "    5. Build forest by repeating steps one to four for *n* times to create *n* number of trees\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1924cd",
   "metadata": {},
   "source": [
    "#### Tuning model parameters, evaluation\n",
    "**Overfitting** referes to a model that is only set for a very small amount of data and ignores the bigger picture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386a1eca",
   "metadata": {},
   "source": [
    "| There are three main methods to avoid it:\n",
    "|   - feature selection\n",
    "|   - cross-validation\n",
    "|   - feature engineering  (creating more data samples using the existing set of data, for egz. In CNN it is producing new images by rotating, scaling, flipping,..)\n",
    "|   - regularization\n",
    "|   - early stopping (regularization technique that identifies the point from where the training data leads to generalization error\n",
    "|   - dropouts (regularization technique used in the case of NN where we randomly deactivate a proportion of neurons in each layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb0647a",
   "metadata": {},
   "source": [
    "* Dimensionality reduction helps in compressing data and removing redundant features. **Feature selection** is the method of reducing the input variable to your model by using only relevant data and getting rid of noise in data.\n",
    "\n",
    "    ![](Picture8.png){fig-align=\"center\"}\n",
    "    - Filter method: features are dropped based on their relation to the output, or how they are correlating to the output\n",
    "    - Wrapper method: we split our data into subsets and train a model using this. Based on the output of th emodel, we add and subtract features and train the model again. It forms the subsets using a greedy approach and evaluates the accuracy of all the possible combinations of features.\n",
    "    \n",
    "    **Multicollinearity** is reflected in the model when independent variables in a multiple regression model are deduced to possess high correlations with each other. It can be overcomed by removing a few highly correlated variables from the equation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6203759",
   "metadata": {},
   "source": [
    "* **Feature scaling** is one of the most important data preprocessing steps in ML. Algorithms that compute the distance between the features are biased towards numerically larger values if the data is not scaled. Most popular are normalizaton and standardization. Also, sklearn library provides transformers *MinMaxScaler* and *StandardScaler*.\n",
    "* **Feature engineering** is the method that is used to create new features from the given dataset using the existing variables. For egz. Imputation, discretization, categorical encoding,..\n",
    "* **Cross-validation** is a statistical method used to estimate the performance of ML models. It is used to protect against overfitting in a  predictive model, particularly in a case where the amount of data may be limited.\n",
    "\n",
    "    **k-fold cross validation** guarantees that the score of our model does not depend on the way we picked the train and test set. The data is first randomly divided into k number of subsets. For each subset in your dataset, build your model on k-1 subsets of the dataset. Then, test the model to check the efectiveness for kth subset. Repeat this until each of k-subsets has served as the test set. The average of your *k* recorded accuracy is called the cross-validation accuracy and will serve you as your performance metric for the model. The disadvantage of this method is that the training algorithm has to be rerun from scratch *k* times. Also, it only estimates the accuracy but does not improve it.\n",
    "\n",
    "* **Regularization** is a form of regression which discourages learning a more complex or flexible model, so as to avoid the risk of overfitting. The general idea is to penalize complicated models by adding an additional penalty to the loss function in order to generate a larger loss. In this way, we can discourage the model from learning too many details and the model is much more general. Three popular methods are Ridge regression (L2 norm, most used), Lasso (L1 norm) and Dropout (used in neural networks). If there is noise in the training data, then estimated coefficients won't generalize well to the future data and this is where regularization comes in. It happens by adding a tuning parameter  Î» that decides how much we want to penalize the flexibility of our model. As the value of Î» rises, it reduces the value of coefficients and thus reducing the variance. Till a point, this increase in  Î» is beneficial as it is only reducing the variance (hence avoiding overfitting), without loosing any important properties in the data. But after certain value, the model starts lossing important properties, giving rise to bias in the model and thus underfitting.\n",
    "* **Ensemble learning** is combining several individual models together to improve performance.\n",
    "\n",
    "    1. **Boosting** is one of the ensemble learning methods where we create multiple models and sequentially train them by combining weak models iteratively in a way that training a new model depends on the models trained before it. We take the patterns learned by a previous model and test them on a dataset when training the new model. In each iteration, we give more importance to observations in the dataset that are incorrectly handled or predicted by previous models. It is useful in reducing bias also.\n",
    "    2. **Bagging** is an ensemble learning method where we generate some data using the bootstrap method, in which we use an already existing dataset and generate multiple samples of the â€žNâ€œ size. This bootstrapped data is then used to train multiple models in parallel, which makes it more robust than a simple model. Once all the models are trained and it is time to make a prediction, we make predictions using all the trained models and then average the result in the case of regression, and for classification, we choose the result that has the highest frequency.\n",
    "    3. **Stacking** is an ensemble learning method where we can combine weak models that can additionaly use different learning algorithms as well. These learners are called heterogeneous learners (boosting and bagging are homogeneous learners). Stacking works by training multiple and different weak models or learners and then using them together by training another model, called a meta-model, to make predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21700ae8",
   "metadata": {},
   "source": [
    "Three commonly used methods for finding the sweet spot between simple and complicated models are: regularization, boosting and bagging.\n",
    "\n",
    "* **Gradient descent,** in ML, is an iterative method that minimizes the cost function parametrized by model parameters. This improves the learning model's efficacy by providing feedback to the model so that it can adjust the parameters to minimize the error and find the local or global minimum. Gradient measures the change in parameter with respect to the change in error. **Learning rate** or step size is the size of the steps that are taken to reach the minimum. This is typically a small value, and it is evaluated and updated based on the behavior of the cost function. High learning rates result in larger steps but risks overshooting the minimum. There are 3 types of gradient descent method:\n",
    "    - **batch gradient descent:** computation is carried out on the entire dataset\n",
    "    - **stochastic gradient descent:** computation is carried over only one training sample\n",
    "    - **mini batch gradient descent:** a small number/batch of training samples is used for computation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af042296",
   "metadata": {},
   "source": [
    "### Deep learning\n",
    "\n",
    "Deep learning is an advanced version of neural networks (NNs with more than three layers) to make the machines learn from data.\n",
    "\n",
    "* **RNN** (reccurent neural network) is an algorithm that uses sequential data (i.e. data that are ordered into sequences) such as timeseries, stock market, temperature, etc.\n",
    "* **NLP** (natural language processing) deals with the study of how computers learn a massive amount of textual data through programming\n",
    "* **Batch normalization** is a technique for training very deep neural networks that standardize the inputs to a layer for each mini-batch. This has the effect of stabilizing the learning process and dramatically reducing the number of training epochs required to train deep networks. After this, model is less sensitive to hyperparameter tuning, high learning rates become acceptable (which results in faster training of the model), weight initialization becomes an easy task,..\n",
    "* A perceptron** is the simplest NN that contains a single neuron that performs 2 functions. The first function is to perform the weighted sum of all the inputs and the second is an activation function\n",
    "* **Autoencoders** are learning networks which transform inputs into outputs with minimum possible errors. Can be used in anomaly detection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a35a37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
