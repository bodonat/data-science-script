[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Script for data science",
    "section": "",
    "text": "Introduction\nWelcome to this thorough guide to Data Science. Here you can find a step-by-step introduction to the world of Data Science and a lot more!\nThe primary goal is to cover basic principles in a detailed manner for better understanding. Also, it can come in handy to remind yourself of some forgotten concepts.\nFirst part covers Python programming and its fundamental basics, along with useful modules and functions. Second part is first concetrated on the background of Data Science, then follows some popular methods and algorithms.\nHave fun! :)"
  },
  {
    "objectID": "python_intro.html#characteristics-of-python-objects",
    "href": "python_intro.html#characteristics-of-python-objects",
    "title": "1  Intro",
    "section": "1.1 Characteristics of Python objects",
    "text": "1.1 Characteristics of Python objects\nIterable, ordered, mutable and hashable (and their opposites) are characteristics for describing Python objects or data types.\n\nList, tuples, dicts and sets are all iterable objects. They are iterable containers which you can get an iterator from. All these objects have iter() method which is used to get an iterator.\nOrdered vs unordered: in unordered types you don’t have direct access to elements (sets, frozensets, ..)\nFrozen set is immutable type of set. Set is a mutable data type since we can modify it by adding or removing items from it.\nImutable and mutable: id and type of an object never changes, but value can either change or not. Mutables are: lists, arrays, sets and dictionaries. Immutables are: numeric data types (integers, and other built-in numeric data such as booleans, floats, complex numbers, fractions and decimals), strings, bytes, frozen sets and tuples.\nHashable is a feature of Python objects that tells if the object has a hash value or not. Hash value is a numeric value of fixed length that uniquely identifies data. If the object has a hash value then it can be used as a key for dictionary or as an element in a set. An object is hashable if it has a hash value that does not change during its entire lifetime. Almost all immutable objects are hashable, i.e. all built-in types has a hash method. Unhashable are: dict, list and set."
  },
  {
    "objectID": "python_intro.html#some-differences",
    "href": "python_intro.html#some-differences",
    "title": "1  Intro",
    "section": "1.2 Some differences",
    "text": "1.2 Some differences\n\nTuples and lists:\n\nTuples are immutable, lists aren’t\nTuples are faster and they consume less memory\nTuples don’t consist of any built-in functions\n\nLists and arrays:\n\nUnlike lists, arrays can only hold a single datatype\nBoth of them have the same way of storing data\nLists are recommended to use for shorter sequence\nFor printing, lists can be printed entirely, but arrays need to loop to be defined to print or access the components\n\nLists and numpy arrays:\n\nLists support insertion, deletion, appending and concatenation, but don’t support vectorized operations like numpy arrays\nList comprehensions make them easy to construct\nNumpy arrays are faster\nThe fact that lists can contain objects of different types mean that Python must store type information for every element"
  },
  {
    "objectID": "python_intro.html#shallow-vs-deep-copy",
    "href": "python_intro.html#shallow-vs-deep-copy",
    "title": "1  Intro",
    "section": "1.3 Shallow vs deep copy",
    "text": "1.3 Shallow vs deep copy\nIn Python, assignment statements do not copy objects, they create bindings between a target and an object. When we use the = operator, it only cretes a new variable that shares the reference of the original object. So, if you edit the new list, changes will be reflected on the original list.\n\n\nCode\n# egz. 1\nlist_A = [1, 2, 3]\nlist_B = list_A\nlist_B.append(4)\n\n# list_A = [1, 2, 3, 4]\n\n\n\n\nCode\n# egz. 2\nlist_of_lists_A = [[1, 2], [3, 4]]\nlist_of_lists_B = list_of_lists_A\n\nlist_of_lists_B.append([5, 6])\n# list_of_lists_B = [[1, 2], [3, 4], [5, 6]]\n# list_of_lists_A = [[1, 2], [3, 4], [5, 6]]\n\n\nIn order to create „real copies“ or „clones“ of these objects, we can use the copy module. A shallow copy creates a new compound object (object that contain other objects, like lists or class instances) and elements in the new object are referenced to the original elements. Changes made in any member of the class will also affect the original copy of it. But, since it creates a new object, changes like adding or removing items won’t affect the original list, i.e. new list has its own pointer, but its elements don’t. In the case of deep copy, a copy of the object is copied into another object. It means that any changes made to a copy of the object do not reflect in the original object. Copy() returns a shallow copy of the list (list[:] also works), and deepcopy() returns the deep copy.\n\n\nCode\n#  shallow copy\nimport copy\n\nlist_of_lists_A = [[1, 2], [3, 4]]\nlist_of_lists_B = copy.copy(list_of_lists_A)\n\nlist_of_lists_B.append([5, 6])\n# list_of_lists_B = [[1, 2], [3, 4], [5, 6]]\n# list_of_lists_A = [[1, 2], [3, 4]]\n\nlist_of_lists_B[1].append(5)\n# list_of_lists_B = [[1, 2], [3, 4, 5], [5, 6]]\n# list_of_lists_A = [[1, 2], [3, 4, 5]]\n\n\n\n\nCode\n# deep copy\nimport copy\n\nlist_of_lists_A = [[1, 2], [3, 4]]\nlist_of_lists_B = copy.deepcopy(list_of_lists_A)\n\nlist_of_lists_B[1].append(5)\n# list_of_lists_B = [[1, 2], [3, 4, 5]]\n# list_of_lists_A = [[1, 2], [3, 4]]"
  },
  {
    "objectID": "python_intro.html#good-practice",
    "href": "python_intro.html#good-practice",
    "title": "1  Intro",
    "section": "1.4 Good practice",
    "text": "1.4 Good practice\n\nLambda=annonymous function. It is similar to the inline function in c programming. It returns a function object and can also be used in the place of a variable.\nTernary operator is one-line version of the if-else statement to test a condition\npass statement is used as a placeholder for future code.\nassert statement allows you to test if certain assumptions remain true while you are developing your code. Assertions are a convenient tool for documenting, debugging and testing code during development. With assertions, you can set checks to make sure that invariants within your code stay invariant. By doing so, you can check assumptions like preconditions and postconditions.\nGenerator is a function that returns an iterator that produces a sequence of values when iterated over. Instead of return statement we use the „yield“ statement. The yield keyword is used to produce a value from the generator and pause the generator function’s execution until the next value is requested. When the generator function is called, it returns a generator object that can be iterated over to produce the values. They are more memory-efficient than storing an entire sequence in memory (for egz. Iterators).\n\n\n\nCode\ndef my_generator(n):\n    value = 0\n    while value &lt; n:\n        yield value\n        value += 1\n\n\n\n\n\n\n\n\nTip\n\n\n\nConcise way for writing a generator is generator expression that looks like a list comprehension."
  },
  {
    "objectID": "handling_exceptions.html",
    "href": "handling_exceptions.html",
    "title": "2  Handling exceptions",
    "section": "",
    "text": "Exception vs error: errors cannot be handled, while exceptions can be catched at the run time. The error indicates a problem that mainly occurs due to the lack of system resources while Exceptions are the problems which can occur at runtime and compile time. It mainly occurs in the code written by the developers. An error can be a syntax error, while there can be many types of exceptions that could occur during the execution. An error might indicate critical problems that a reasonable application should not try to catch, while an exception might indicate conditions that an application should try to cacth.\n\n\n\n\n\n\n\nNote\n\n\n\nPython uses float because with binary representation we can’t represent decimal numbers. Every number is actually some approximation of some other number, and difference between representation and real value is called round-off error.\n\n\n\nTry-except-else: the try block lets you test a block of code for errors. The except block gets executed when the error occurs. The else block lets you execute code when there is no error.\nTry-except-finally: finally statement is opposite of „else“. It always executes after try and except blocks. It is used to do the clean up activities of objects/variables."
  },
  {
    "objectID": "oop.html#basic-building-elements-of-oop",
    "href": "oop.html#basic-building-elements-of-oop",
    "title": "3  Object oriented programming",
    "section": "3.1 Basic building elements of OOP",
    "text": "3.1 Basic building elements of OOP\n\nInheritance provides code reusability. We have single, multi-level, multiple (more than one base class) and hierarchical (when more than one derived class are created from a single base)\n\n\n\nCode\n# egz. of inheritance and use of super() function\n\nclass MyClass():\n    def __init__(self, x):\n        print(x)\n\nclass SubClass(MyClass):\n    def __init__(self, x, y):\n        self.y = y\n        super().__init__(x) \n\n\n\nPolymorphism means the ability to take multiple forms. So if the parent class has a method named ABC then the child class also can have a method with the same name ABC having its own parameters and variables.\nEncapsulation is a process of wrapping data and functions that perform actions on the data into a single entity. A single unit is referred to as a class. To access the values, the class usually provides publicly accessible methods (setters and getters). Technique that hides implementation details.\nAbstraction is used to hide something too, but in a higher degree (class, interface). Clients who use an abstract class do not care about what it was, they just need to know what it can do."
  },
  {
    "objectID": "oop.html#methods-classes",
    "href": "oop.html#methods-classes",
    "title": "3  Object oriented programming",
    "section": "3.2 Methods, classes",
    "text": "3.2 Methods, classes\n\ninit is a method that is automatically called to allocate memory when a new object (i.e. instance of a class) is created. It acts as a constructor which gets executed when a new object is instantiated and allows the class to classify its attributes.\nself is an object of a class. The self variable in the init method refers to the newly created object, while in other methods it refers to the object whose method was called. It is used to refer to the object properties of a class.\nobject() returns featureless object that is a base for all classes\n\n\n\n\n\n\n\nNote\n\n\n\nAs Python has no concept of private variables, leading underscores are used to indicate variables that must not be accessed from outside the class.\n\n\n\nNamed Tuple can be a great alternative to construct a class. It is an extension of the Python built-in tuple data type, which is structure for grouping objects with different types. When you access an attribute of the built-in tuple, you need to know its index. Named Tuple allows us to give names to the elements, so we can access the attributes by both attribute name and its index. It is good practice to use classes constructed like this when we have a function that takes more than 3 arguments, which is too much. Then it is better to pack most of the arguments into a class.\n\n\n\nCode\nfrom typing import NamedTuple\n\nclass Transaction(NamedTuple):\n    sender: str\n    receiver: str\n    date: str\n\n\n\nClass attributes belong to every instance of some class. They are defined outside of init function. So they are different from instance attributes.\nDecorator is a design pattern that allows a user to add new functionality to an existing object without modifying its structure. They are usually called before the definition of a function you want to decorate. Decorator takes in a function and returns it by adding some functionality. A few good examples for using decorators are when you want to add logging, test performance, perform caching, verify permissions…\n\n\n\nCode\ndef make_pretty(func):\n    def inner():\n        print(\"I got decorated\")\n        func()\n    return inner\n\n@make_pretty\ndef ordinary():\n    print(\"I am ordinary\")\n\n# ordinary()\n# Output: \n#   I got decorated\n#   I am ordinary"
  },
  {
    "objectID": "useful_functions.html#working-with-files",
    "href": "useful_functions.html#working-with-files",
    "title": "4  Useful functions",
    "section": "4.1 Working with files",
    "text": "4.1 Working with files\n\n“with open(„some.txt“) as f” takes care of closing the file\n.write(), .read(), .close(), .readlines()\nFor faster loading it is commonly to use numpy package: np.savetxt(‘some.txt’, np.array(), fmt=‘%.2f’, header=‘..’), np.loadtxt(‘some.txt’)"
  },
  {
    "objectID": "useful_functions.html#working-with-dataframes",
    "href": "useful_functions.html#working-with-dataframes",
    "title": "4  Useful functions",
    "section": "4.2 Working with dataframes",
    "text": "4.2 Working with dataframes\n\nCombining dataframes in pandas: append() (for horizontal stacking) and concat() (for vertical stacking)\nIdentifying missing values: isnull() and isna()\nHandling missing values: fillna()\nCreating missing values for known indexes: pd.DataFrame(index=…, columns=…)\ndf.query(): for extracting data with specified conditions\nWhen you want to create a new df based on a subset of your initial df, it’s best to use the copy() method\nPandas also has str() method\nRenaming columns: better to use .rename() function\nSetting index to a column: df.set_index([‘some’]) (it removes column automatically)\nDataFrame.plot makes plots of Series or DataFrame. There is various kinds of plot to produce and can be assigned in „kind“ parameter. Options are: ‘line’ (default), ‘bar’, ‘barh’, ‘hist’, ‘box’, ‘kde’/‘density’, ‘area’, ‘pie’, ‘scatter’, ‘hexbin’’"
  },
  {
    "objectID": "python_modules.html",
    "href": "python_modules.html",
    "title": "5  Modules",
    "section": "",
    "text": "itertools module, for different iterations\ncounter is a dict subclass for counting hashable objects. It is a collection where elements are stored as dictionary keys and their counts are stored as dictionary values. Counts are allowed to be any integer value including zero or negative counts. It is a Pythonic way to count objects.\nfrom collections import Counter\n\nupdate(): for adding elements\nelements(): for restoring elements (Counter remembers the insertion order of its keys as a feature inherited from dict)\nsubtract(): for removing elements\nmost_common(), &, |, +, -\n\nFile related modules: os, os.path, shutil.os\nPickle module accepts any Python object and converts it into a string representation and dumps it into a file by using dump function. This process is called pickling. While the process of retrieving original Python objects from the stored string representation is called unpickling.\noperator module exports a set of efficient functions corresponding to the intrinsic operators of Python. The functions fall into categories that perform object comparisons, logical operations, mathematical operations and sequence operations. This functions are handy in cases where callables must be stored, passed as arguments (egz. For map(), sorted(), itertools, groupby()), or returned as function results. Probably the most used function is .itemgetter() with which you can sort some list of tuples, or dict,.. with specified key, and is also faster than sort function. It is also more clearable than lambda function when sending as an argument\negz. If you want to sort some lists/dict first by 2nd element and then by 1st element, you can use key=itemgetter(1,0).\nnumpy stands for numerical python and it is used for general numeric computations on numerical data saved in arrays. Provides vectorization of mathematical operations on arrays and matrices\nscipy stands for scientific python. Collection of algorithms for linear algebra, solving differential equations, numerical integration, optimization, statistics and more.. (built on numpy)\npandas adds data structures and tools designed to work with table-like data. Provides tools for data manipulation: reshaping, merging, sorting, slicing, aggregations.. allows handling missing data. Used to implement ETL (extracting, transforming and loading the datasets)\nsklearn provides machine learning algorithms: classification, regression, clustering, model validation, etc… (built on numpy, scipy and matplotlib)\nmatplotlib is 2D plotting library which produces publication quality figures in a variety of hardcopy formats. A set of functionalities similar to those of MATLAB. Has line plots, scatter plots, barcharts, histograms, pie charts, etc..\nseaborn provides high level interface for drawing attractive statistical graphs.. (based on matplotlib)"
  },
  {
    "objectID": "ds_intro.html",
    "href": "ds_intro.html",
    "title": "6  Intro",
    "section": "",
    "text": "Data science combines statistics, maths, specialised programs, AI, ML, etc.. Application of specific principles and analytic techniques to extract information from data used in strategic planning, decision making, etc.\n\n\n\n\n\n\n\nds_cycle\n\n  \n\nA\n\n  Business Understanding     Define the problem   to be solved   \n\nB\n\n  Data Mining     Gather or scrape  the necessary data   \n\nA-&gt;B\n\n    \n\nC\n\n  Data Cleaning     Handle missing values  and outliers   \n\nB-&gt;C\n\n    \n\nD\n\n  Data Exploration     Visual analysis of data   \n\nC-&gt;D\n\n    \n\nE\n\n  Feature engineering     Create new features  from the dataset   \n\nD-&gt;E\n\n    \n\nF\n\n  Predictive modeling     Train ML models,  evaluate and make                  predictions   \n\nE-&gt;F\n\n    \n\nG\n\n  Data Visualization     Represent the findings  using plots and                  interactive visualizations   \n\nF-&gt;G\n\n   \n\n\n\n\n\n\n\n\nHandling missing values: If the dataset is large, we can just remove rows with missing data. For smaller datasets, we can substitute it with the mean or average, using df.mean(), or df.fillna(mean),..\nHandling outliers: if the data has garbage value or has extreme values you can drop it. Otherwise, you can maybe try different models, normalize data or use algorithms that are less affected by it\nDifference between error terms and residuals: an error term is generally unobservable and a residual is observable and calculable, making it much easier to quantify and visualize. In effect, while an error term represents the way observed data differs from the actual population, a residual represents the way it differs from sample population data. Residuals actually helps us get an accurate estimate of the error.\nRMSE is used to measure the deviation of the residuals, and MSE is used to find how close is the line to the actual data. Other metrics: MAE (mean absolute error), MAPE (mean absolute percentage error). from sklearn.metrics import …….\nCost vs loss function: loss function refers to the error of one training example, while a cost function calculates the average error across an entire training set\nConfusion matrix is used to describe the performance of the classification model.\n\n\n\n\nActual Positive\nActual Negative\n\n\n\n\nPredicted Positive\nTrue Positive\nFalse Positive\n\n\nPredicted Negative\nFalse Negative\nTrue Negative\n\n\n\n\naccuracy: (true positive + true negative) / total observations.. RMSE is a measure of accuracy in regression\nerror rate: (false positive + false negative) / total observations\nprecision: true positive / (true positive + false positive)\nspecificity: true negative / (true negative + false positive)\nsensitivity/recall: true positive / (true positive + false negative).. helps to identify the misclassified positive predicitions\n\nAccuracy metric can be reliable metric only if the dataset is class-balanced. F1 score is a ML evaluation metric that assesses the predictive skill of a model by elaborating on its class-wise performance rather than an overall performance as done by accuracy. If accuracy is 100%, then F1 = 1.\n\\[ F1 = 2 * \\text{precision} * \\text{recall} / (\\text{precision + recall}) \\]\nCut off/threshold is the probability that the prediction is true. It represents the tradeoff between false positives and false negatives. Normally, the cut-off will be on 0.5 (random) but you can increase it. All predicted outcome with a probability above it will be classified in the first class and the other in the second class."
  },
  {
    "objectID": "statistics.html",
    "href": "statistics.html",
    "title": "7  Statistics",
    "section": "",
    "text": "Deterministic vs stochastic process: a deterministic process is a mathematical model where the output depends solely on the input, and there is no randomness involved. In contrast, a stochastic process is a mathematical model that involves randomness and is used to model situations that may not have inherent randomness. A deterministic model is completely predictable also.\nUnit root is a feature of some stochastic processes. A linear stochastic process has a unit root if 1 is a root of the process’s characteristic equation. Such a process is non-stationary. If the other roots of the characteristic equation lie inside the unit circle, then the first difference of the process will be stationary; otherwise, the process will need to be differenced multiple times to become stationary.\nBias is a systematic tendency to underestimate or overestimate the value of a parameter (you were not random!). It implies that the data selection may have been skewed by the collection criteria (in favor or against an idea). It can also be defined as a systematic (built-in) error which makes all values wrong by a certain amount. In ML, the inability for a ML method to capture the true relationship is called bias, that happens because algorithm makes simplified assumptions so that it can easily understand the target function.\n\n\n\n\n\n\n\n\nBias\nError\n\n\n\n\nProduces prejudiced results\nResults in inaccurate outcomes\n\n\nIdentified manually or through software packages\nIdentified through calculations\n\n\nOccurs systematically\nOccurs randomly\n\n\n\n\nNormal/uniform distribution is the kind of distribution that has no bias either to the left or to the right and is in the form of a bell-shaped curve. In this distribution, mean is equal to the median.\nSkewed distribution is a distribution where the curve is inclined towards one side.\nVariance is a statistical measurement used to determine the average of each point from the mean (the average of the squared differences from the mean). In ML the difference in fits between training and test sets is called variance, i.e. it refers to the changes in the model when using different portions of the training data set. Simply put, variance is the variability in the model prediction. Standard deviation is the spread of a group of numbers from the mean.\n\n\\[ Var(X) = E[X^2] – E[X]^2 \\]\n\n\n\n\n\n\n\nSigns of high bias ML model\nSigns of high variance ML model\n\n\n\n\nFailure to capture data trends\nNoise in data set\n\n\nUnderfitting\nOverfitting\n\n\nOverly simplified\nComplexity\n\n\nHigh error rate\nForcing data points together\n\n\n\n\nRobustness represents the system’s capability to handle differences and variances effectively\nTotal error = variance + bias + irreducible error\n\n\n\n\n\n\n\n\n\n\n\nCorrelation vs covariance: correlation is a measure of relationship between two variables and says how strong are the variables related. Range is -1 to 1. Covariance represents the extent to which the variables change together in a cycle. This explains the systematic relationship between pair of variables where changes in one affect changes in another variable. Range is -inf to +inf, and is affected by scalability.\nConfounding variables are extraneous variables in a statistical model that correlates directly or inversely with both the dependent and the independent variable. Left unchecked, confounding variables can introduce many research biases to your work, causing you to misinterpret your results.\nR-squared/coefficient of determination is a statistical measure in a linear regression model that determines the proportion (percentage) of the variance in the dependent variable that can be explained by the independent variable. In other words, it evaluates the scatter of the data points around the fitted regression line, i.e. shows how well the regression model explains observed data.\n\n\\[\\begin{aligned}\nR^{2} &= 1 - \\frac{\\text{Residual variance}}{\\text{Total variance}} \\\\\n&=\\frac{\\text{Total variance - Residual variance}}{\\text{Total variance}} \\\\\n&=\\frac{\\text{Explained variance}}{\\text{Total variance}} \\\\\n&=\\text{Fraction of total variance explained}\n\\end{aligned}\\]\n\np-value is the measure of the statistical importance of an observation. We compute the p-value to understand whether the given data really describes the observed effect or not. If p&lt;=0.05 it suggests that there is only 5% chance that the outcomes of an experiment are random and the null hypothesis must be rejected. \\[ p_{value} = P(E|H_{0}) \\]\nThe statistical power of a binary hypothesis test is the probability that the test rejects the null hypothesis when a specific alternative hypothesis is true\nConfidence interval is a range of values likely containing the population parameter. Confidence level is denoted by 1-\\(\\alpha\\), where \\(\\alpha\\) is level of significance (usually 5%). Point estimate is an estimate of the population parameter (can be derived with Maximum Likelihood estimator for egz.) \nUnivariate, bivariate and multivariate analysis: univariate analysis allows us to understand the data and extract patterns and trends out of it. Bivariate analysis allows us to figure out the relationship between the variables. Multivariate analysis allows us to figure out the effects of all other variables (input variables) on a single variable (the output variable).\nSampling is the selection of individual members or a subset of the population to estimate the characters of the whole population. It is useful with datasets that are too large to efficiently analyze in full. There are two types of sampling techniques: probability and non-probability. Resampling is the process of changing/exchanging data samples, identifying the impact of these changes on model and prediction characteristics, and continuing until optimal results are achieved. It is done in cases of estimating the accuracy of sample statistics or validating models by using random subsets to ensure variations are handled (egz. Bootstraping, cross-validation)\nTypes of biases that can occur during sampling: selection bias, undercoverage bias and survivorship bias. Selection bias occurs when a sample selection does not accurately reflect the target population. Survivorship bias is the logical error of focusing on aspects that support surviving a process and casually overlooking those that did not. This can lead to wrong conclusions in numerous ways.\nBootstrap method is a resampling method by independently sampling with replacement from an existing sample data with same sample size n, and performing inference among these resampled data.\nNormalization/Min-Max scaling is used to transform features to be on a similar scale ([0,1] or [-1,1]). It is useful when there are no outliers. \\[ X_{new} = (X - X_{min}) / (X_{max} - X_{min}) \\] Normalization is useful when your data have different dimensions and the method you’re employing doesn’t make assumptions about the distirbution of you data.\nStandardization/Z-score normalization is the transformation of features by subtracting from mean and dividing by standard deviation. It is not affected with outliers since its not bounded to a certain range. Changing the range of your data with scaling is different from changing the distribution of your data with Normalization. Also, standardization presupposes that the distribution of your data is Gaussian. \\[ z = \\frac{x-\\mu}{\\sigma}, \\] where \\(\\mu\\) represents the mean and \\(\\sigma\\) represents the standard deviation.\nThe Central limit theorem says that, given a large enough sample size, the distirbution of sample averages/means will be approximtely normal. This means that we can use normal distirbution to make predictions about populations based on samples.\nThe Law of large numbers is a theorem that describes the result of performing the same experiment very frequently. It states that the sample mean, sample variance, and sample standard deviation converge to what they are trying to estimate\nGradient, for purposes of this paper, measures how much the output of a function changes if you change the inputs a little bit (from a given point).\nCategorical, continuous and ordinal variables. An ordinal variable is a categorical variable for which the possible values are ordered.\nHistrograms vs boxplots: Boxplots are more often used in comparing several datasets and take less space than histograms. Histograms are used to know and understand the probability distribution underlying a dataset\nHistograms vs bar graphs: a bar graph is the graphical representation of categorical data, whereas a histogram is the graphical representation of data where data is grouped into continuous number ranges\nKernel density estimation (KDE) is a method for visualizing the distirbution of observations in dataset over a continuous interval or time period\nHistograms vs density plots: an advantage that density plots have over histograms is that they’re better at determining the distribution shape because they are not affected by the number of bins used.\nAkaike information Criterion (AIC): is a mathematical method for evaluating how well a model fits the data it was generated from. In statistics, it is used to compare different possible models (model selection). Lower AIC scores are better!"
  },
  {
    "objectID": "tsa.html#models",
    "href": "tsa.html#models",
    "title": "8  Time series analysis",
    "section": "8.1 Models",
    "text": "8.1 Models\n\nAutoRegressive model (AR): it is a linear model where current period values are a sum of past outcomes multiplied by a numeric factor. We denote it as AR(p), where „p“ is called the order of the model and represents the number of lagged values we want to include. p can be determined from PACF plot. For p=1: \\[ X_{t} = C + \\phi_{1}X_{t-1} + \\varepsilon_{t}, \\] The coefficient \\(\\phi_{1}\\) is a numeric constant with value between -1 and 1. When multiplied with past value it represents a part which remains in the future. You would choose an AR model if you believe that previous observations have a direct effect on the time series.\nMoving Average (MA): it’s a statistic that captures the average change in data series over time. We denote it as MA(q), where „q“ is called the order of the model and represents the number of past forecast errors (or the size of the moving average window). q can be determined from ACF plot. You would choose an MA model if you believe that the errors have a direct effect on the time series.\nAutoRegressive Moving Average (ARMA): p,q\nAutoRegressive Integrated Moving Average (ARIMA): p,d,q.. where d is the difference order\nAutoRegressive Moving Average with eXogeneous factors (ARMAX): exogeneous variables are external data used in forecast (external effects)\nSeasonal AutoRegressive Integrated Moving Average (SARIMA): p,d,q,P,D,Q,m.. where m is the number of time steps for a single seasonal period, p,d,q are trend elements and P,D,Q are seasonal elements\nSeasonal AutoRegressive Integrated Moving Average with eXogeneous factors (SARIMAX)\nMoving Average (MA): it’s a statistic that captures the average change in data series over time. We denote it as MA(q), where „q“ is called the order of the model and represents the number of past forecast errors (or the size of the moving average window). q can be determined from ACF plot. You would choose an MA model if you believe that the errors have a direct effect on the time series.\nAutoRegressive Moving Average (ARMA): p,q\nAutoRegressive Integrated Moving Average (ARIMA): p,d,q.. where d is the difference order\nAutoRegressive Moving Average with eXogeneous factors (ARMAX): exogeneous variables are external data used in forecast (external effects)\nSeasonal AutoRegressive Integrated Moving Average (SARIMA): p,d,q,P,D,Q,m.. where m is the number of time steps for a single seasonal period, p,d,q are trend elements and P,D,Q are seasonal elements\nSeasonal AutoRegressive Integrated Moving Average with eXogeneous factors (SARIMAX)\n\n\n8.1.1 Steps for building a model\n\nCheck for stationarity of time series and perform differencing if needed. This is because the term „autoregressive“ implies Linear Regression model (using its lags as predictors) and it works well for independent and non-correlated predictors\nDetermine parameters. It can be done with inspecting acf/pacf plots\nFit the model. Inspect coefficients and P(&gt;|z|) with .summary() function and decide if it is needed for further tuning of parameters\nCheck residuals for making sure model has captured adequte information from the data (they should look like white noise). If density looks normally distirbuted, model is ready.\nMake predictions (using .forecast() or .predict() function)\nEvaluate model predictions using common metrics (MAE, RMSE,..)\n\nSuggestions\n\nAlternatively, use pmdarima package and auto_arima function to automate steps 1 to 3. Be aware that sometimes the manually fitted model is closer to the actual test set\nAlternatively, use plot_diagnostics to automate step 4. Values of good fit:\n\nStandardized residual: there are no obvious patterns in residuals, with values having a mean of zero\nThe KDE curve should be very similar to the normal distribution\nNormal Q-Q: most of the data points should lie on the straight line\nCorrelogram: 95% of correlations for lag greater than zero should not be significant\n\nSuggestion: conduct time series cross-validation to select the best model, i.e. repeat model assessment for different train / test sets\nSuggestion: if data shows exponential trend you can do a log transform before applying a model, then later apply inverse transformation (exponential function)\n\nUseful tips/functions\n\nDate increment used for a date range: pandas.tseries.offsets.DateOffset"
  },
  {
    "objectID": "machine_learning.html#regression-and-classification-algorithms",
    "href": "machine_learning.html#regression-and-classification-algorithms",
    "title": "9  Machine learning",
    "section": "9.1 Regression and classification algorithms",
    "text": "9.1 Regression and classification algorithms\n\nRegression: linear (when variables are continuous and numeric) and logistic (when variables are continuous and categorical)\n\nLinear regression is supervised learning algorithm, which helps in finding the linear relationship between two variables. It finds the smallest sum of squared residuals that is possible for the dataset.\n\nClassification refers to a predictive modeling process where a class label is predicted for a given example of input data. It helps categorize the provided input into a label that other observations with similar features have.\n\nNaive Bayes is supervised classification ML algorithm based on the Bayes theorem, which deals with the probability of an event occuring given that another event has already occured (i.e. mathematical formula for determining conditional probability). It is based on two assumptions, first, each feature/attribute present in the dataset is independent of another, and second, each feature carries equal importance. It has „naive“ in it because it assumes that the occurence of a certain feature is independent of the occurence of other features (hence each feature individually contributes to identify the result), which is unrealistic for real-world data\nSupport vector machine (SVM) is a supervised ML model that considers the classification algorithms for two-group classification problems. It is a representation of the training data as points in space that are seperated into categories with the help of a clear gap that should be as wide as possible. Kernel function is used to transform the data that is not linearly separable into one that is. It is generalized dot product function used for the computing dot product of vectors xx and yy in high dimensional feature space. This transformation is based on kernel trick (projecting data onto a higher dimension space where it can be linearly divided by a plane).\nLogistic regression is classification algorithm that is used to predict the probability of certain classes based on some dependent variables. Estimating probability is done by using its underlying logistic function (sigmoid). In short, the logistic regression model computes a sum of the input features and calculates the logistic of the result.\nAltough it is classiciation algorithm (it predicts a discrete class), it is part of the regression family as it involves predicting outcomes based on quantitative relationships between variables. Unlike, linear regression, it accepts both continuous and discrete variables as input and its output is qualitative.\nSigmoid function: \\(\\quad S(x) = \\frac{1}{1 + e^{-x}} = \\frac{e^{x}}{e^{x} + 1} = 1 - S(-x).\\)\n\nElbow method is used to select „k“ for k-means clustering. It plots the value of the cost function produced by different values of k (for egz. 1 to 15). k-means cost function is sum of squared distances of each data point to respective centroid of cluster to which the data points belong.\nA ROC curve is a graph showing the performance of a classification model at all classification thresholds ([0,1]). It plots two parameters: true positive rate (sensitivity) and false positive rate (1-specificity). Also, decreasing the threshold moves up along the curve. Classifiers that give curves closer to the top-left corner indicate a better performance. Note that the ROC does not depend on the class distribution and this makes it useful for evaluating classifiers predicting rare events such as diseases or disasters. In contrast, evaluating performance using accuracy would favor classifiers that always predict a negative outcome for rare events. To compare different classifiers, it is useful to summarize the performance of each classifier into a single measure- AUC (area under the ROC curve). The AUC is the probability the model will score a randomly chosen positive class higher than a randomly chosen negative class.\nCART is name for classification and regression trees\nDecision tree is non-parametric model that can be used for both classification and regression. Non-parametric means that they don’t increase their number of parameters as we add more features. They are constructed using nodes and branches, where the root node testes a feature which best splits the data. Decision trees are built by recursively splitting our training samples using the features from the data that work best for the specific task. The process is done by evaluating certain metrics („information entropy“), depending if the feature is dicrete or continuous.\nSteps are:\n\nTake the entire data set as input\nLook for a split that maximizes the separation of the classes\nApply the split (divide step)\nRe-apply steps 1) and 2) to the divided data\nStop when you meet stopping criteria\nPruning (clean up the tree if you went too far)\n\nEntropy in ML is the measurement of disorder or impurities in the information processed.\n\\[ E = -\\sum^{N}_{i=1}P_{i}\\text{log}_{2}P_{i}, \\] where \\(P_{i}\\) is probability of randomly selecting an example in class i.\nInformation gain is a measure of how much entropy is reduced when a particular feature is used to split the data. It calculates the difference between entropy before and after the split.\nPruning is a technique that simplifies the decision tree by reducing the rules. It helps to avoid the complexity and improves accuracy.\nRandom forest is a model built up of a number of decision trees. If you split the data into different packages and make a decision tree in each of the different groups of data. The random forest brings all those trees together (individual trees need to have low correlations with each other).\nSteps to build a model:\n\nRandomly select k features from a total of m features (k&lt;&lt;m)\nAmong the k features, calculate the node using the best split point\nSplit the node into daughter nodes using the best split\nRepeat steps two and three until leaf nodes are finalized\nBuild forest by repeating steps one to four for n times to create n number of trees"
  },
  {
    "objectID": "machine_learning.html#tuning-model-parameters-evaluation",
    "href": "machine_learning.html#tuning-model-parameters-evaluation",
    "title": "9  Machine learning",
    "section": "9.2 Tuning model parameters, evaluation",
    "text": "9.2 Tuning model parameters, evaluation\nOverfitting referes to a model that is only set for a very small amount of data and ignores the bigger picture.\nThere are three main methods to avoid it:\n  - feature selection\n  - cross-validation\n  - feature engineering (creating more data samples using the existing set of data, for egz. In CNN it is producing new images by rotating, scaling, flipping,..)\n  - regularization\n  - early stopping (regularization technique that identifies the point from where the training data leads to generalization error\n  - dropouts (regularization technique used in the case of NN where we randomly deactivate a proportion of neurons in each layer)\n\nDimensionality reduction helps in compressing data and removing redundant features. Feature selection is the method of reducing the input variable to your model by using only relevant data and getting rid of noise in data.\n\n\n\n\n\n\n\n\ntree\n\n  \n\nA\n\n Feature selection   \n\nB1\n\n Supervised   \n\nA-&gt;B1\n\n    \n\nB2\n\n Unsupervised   \n\nA-&gt;B2\n\n    \n\nC1\n\n Intrinsic   \n\nB1-&gt;C1\n\n    \n\nC2\n\n Wrapper  method   \n\nB1-&gt;C2\n\n    \n\nC3\n\n Filter  method   \n\nB1-&gt;C3\n\n   \n\n\n\n\n\n\n\\(~~~~~~~~\\)- Filter method: features are dropped based on their relation to the output, or how they are correlating to the output\n\\(~~~~~~~~\\)- Wrapper method: we split our data into subsets and train a model using this. Based on the output of the model, we add and subtract features and train the model again. It forms the subsets using a greedy approach and evaluates the accuracy of all the possible combinations of features.\n\nMulticollinearity is reflected in the model when independent variables in a multiple regression model are deduced to possess high correlations with each other. It can be overcomed by removing a few highly correlated variables from the equation.\nFeature scaling is one of the most important data preprocessing steps in ML. Algorithms that compute the distance between the features are biased towards numerically larger values if the data is not scaled. Most popular are normalizaton and standardization. Also, sklearn library provides transformers MinMaxScaler and StandardScaler.\nFeature engineering is the method that is used to create new features from the given dataset using the existing variables. For egz. Imputation, discretization, categorical encoding,..\nCross-validation is a statistical method used to estimate the performance of ML models. It is used to protect against overfitting in a predictive model, particularly in a case where the amount of data may be limited.\nk-fold cross validation guarantees that the score of our model does not depend on the way we picked the train and test set. The data is first randomly divided into k number of subsets. For each subset in your dataset, build your model on k-1 subsets of the dataset. Then, test the model to check the efectiveness for kth subset. Repeat this until each of k-subsets has served as the test set. The average of your k recorded accuracy is called the cross-validation accuracy and will serve you as your performance metric for the model. The disadvantage of this method is that the training algorithm has to be rerun from scratch k times. Also, it only estimates the accuracy but does not improve it.\nRegularization is a form of regression which discourages learning a more complex or flexible model, so as to avoid the risk of overfitting. The general idea is to penalize complicated models by adding an additional penalty to the loss function in order to generate a larger loss. In this way, we can discourage the model from learning too many details and the model is much more general. Three popular methods are Ridge regression (L2 norm, most used), Lasso (L1 norm) and Dropout (used in neural networks). If there is noise in the training data, then estimated coefficients won’t generalize well to the future data and this is where regularization comes in. It happens by adding a tuning parameter λ that decides how much we want to penalize the flexibility of our model. As the value of λ rises, it reduces the value of coefficients and thus reducing the variance. Till a point, this increase in λ is beneficial as it is only reducing the variance (hence avoiding overfitting), without loosing any important properties in the data. But after certain value, the model starts lossing important properties, giving rise to bias in the model and thus underfitting.\nEnsemble learning is combining several individual models together to improve performance.\n\nBoosting is one of the ensemble learning methods where we create multiple models and sequentially train them by combining weak models iteratively in a way that training a new model depends on the models trained before it. We take the patterns learned by a previous model and test them on a dataset when training the new model. In each iteration, we give more importance to observations in the dataset that are incorrectly handled or predicted by previous models. It is useful in reducing bias also.\nBagging is an ensemble learning method where we generate some data using the bootstrap method, in which we use an already existing dataset and generate multiple samples of the „N“ size. This bootstrapped data is then used to train multiple models in parallel, which makes it more robust than a simple model. Once all the models are trained and it is time to make a prediction, we make predictions using all the trained models and then average the result in the case of regression, and for classification, we choose the result that has the highest frequency.\nStacking is an ensemble learning method where we can combine weak models that can additionaly use different learning algorithms as well. These learners are called heterogeneous learners (boosting and bagging are homogeneous learners). Stacking works by training multiple and different weak models or learners and then using them together by training another model, called a meta-model, to make predictions.\n\n\nThree commonly used methods for finding the sweet spot between simple and complicated models are: regularization, boosting and bagging.\n\nGradient descent, in ML, is an iterative method that minimizes the cost function parametrized by model parameters. This improves the learning model’s efficacy by providing feedback to the model so that it can adjust the parameters to minimize the error and find the local or global minimum. Gradient measures the change in parameter with respect to the change in error. Learning rate or step size is the size of the steps that are taken to reach the minimum. This is typically a small value, and it is evaluated and updated based on the behavior of the cost function. High learning rates result in larger steps but risks overshooting the minimum. There are 3 types of gradient descent method:\n\nbatch gradient descent: computation is carried out on the entire dataset\nstochastic gradient descent: computation is carried over only one training sample\nmini batch gradient descent: a small number/batch of training samples is used for computation"
  },
  {
    "objectID": "deep_learning.html",
    "href": "deep_learning.html",
    "title": "10  Deep learning",
    "section": "",
    "text": "Deep learning is an advanced version of neural networks (NNs with more than three layers) to make the machines learn from data.\n\nRNN (reccurent neural network) is an algorithm that uses sequential data (i.e. data that are ordered into sequences) such as timeseries, stock market, temperature, etc.\nNLP (natural language processing) deals with the study of how computers learn a massive amount of textual data through programming\nBatch normalization is a technique for training very deep neural networks that standardize the inputs to a layer for each mini-batch. This has the effect of stabilizing the learning process and dramatically reducing the number of training epochs required to train deep networks. After this, model is less sensitive to hyperparameter tuning, high learning rates become acceptable (which results in faster training of the model), weight initialization becomes an easy task,..\nA perceptron is the simplest NN that contains a single neuron that performs 2 functions. The first function is to perform the weighted sum of all the inputs and the second is an activation function\nAutoencoders are learning networks which transform inputs into outputs with minimum possible errors. Can be used in anomaly detection."
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "Summary",
    "section": "",
    "text": "Altough we have covered a range of topics, it is worth noting that Data Science goes a lot beyond. It is a filed that is continually evolving and requires constant learning. Because of that further expansion of this guide is desirable and hopefully achievable.\nI hope this script served as a starting point for you, and that it can always come in handy.\nBye :)"
  }
]